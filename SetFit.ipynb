{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO21WTO5axcuIf/sD9iMKqm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashev2021/Fine-Tuning-LLM/blob/main/SetFit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4jI4GMTLqbWh"
      },
      "outputs": [],
      "source": [
        "!pip install \"datasets>=2.18.0,<3\"\n",
        "# deploy in Gradio\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers>=4.38.2 sentence-transformers>=2.5.1 setfit>=1.0.3 accelerate>=0.27.2 seqeval>=1.2.2"
      ],
      "metadata": {
        "id": "cROjwYEGqliQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "from setfit import sample_dataset\n",
        "\n",
        "# Prepare data and splits\n",
        "imdb_data = load_dataset(\"mteb/imdb\")\n",
        "\n",
        "train_data, test_data = imdb_data[\"train\"], imdb_data[\"test\"]\n",
        "\n",
        "\n",
        "train_data = train_data.shuffle(seed=42).select(range(2000))\n",
        "test_data = test_data.shuffle(seed=42).select(range(1000))\n",
        "\n",
        "\n",
        "# We simulate a few-shot setting by sampling 16 examples per class\n",
        "sampled_train_data = sample_dataset(imdb_data[\"train\"], num_samples=16)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wXidFfCdqt7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from setfit import SetFitModel\n",
        "\n",
        "# Load a pre-trained SentenceTransformer model\n",
        "model = SetFitModel.from_pretrained(\"sentence-transformers/distilbert-base-nli-mean-tokens\")"
      ],
      "metadata": {
        "id": "6U4OL_2ZrXCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model with F1\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Calculate F1 score\"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    load_f1 = evaluate.load(\"f1\")\n",
        "    f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
        "    return {\"f1\": f1}\n"
      ],
      "metadata": {
        "id": "dgRDtXXgrqr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from setfit import TrainingArguments as SetFitTrainingArguments\n",
        "from setfit import Trainer as SetFitTrainer\n",
        "\n",
        "# Define training arguments\n",
        "args = SetFitTrainingArguments(\n",
        "    num_epochs=2, # The number of epochs to use for contrastive learning\n",
        "    num_iterations=20  # The number of text pairs to generate\n",
        ")\n",
        "args.eval_strategy = args.evaluation_strategy\n",
        "\n",
        "# Create trainer\n",
        "trainer = SetFitTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=sampled_train_data,\n",
        "    eval_dataset=test_data,\n",
        "    metric=\"f1\"\n",
        ")"
      ],
      "metadata": {
        "id": "PKtV8cH3r5Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.init(project=\"your-project\", name=\"run-1\")\n",
        "\n",
        "\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "rEDTcCLur_pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "fTUoWmdmshw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save fine-tuned model\n",
        "trainer.model.save_pretrained(\"SetFit_finetuned_model\")\n"
      ],
      "metadata": {
        "id": "MLiIt0tjAKPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from setfit import SetFitModel\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load your fine-tuned SetFit model\n",
        "fine_tuned_model = SetFitModel.from_pretrained(\"./SetFit_finetuned_model\")  # Use \"./\" for local\n",
        "\n",
        "# Load a pretrained sentiment model\n",
        "pretrained_classifier = pipeline(\"sentiment-analysis\", model=\"sentence-transformers/distilbert-base-nli-mean-tokens\")\n",
        "\n",
        "# Label map for display purposes (adjust to your modelâ€™s labels if different)\n",
        "label_map = {\n",
        "    \"LABEL_0\": \"negative\",\n",
        "    \"LABEL_1\": \"positive\",\n",
        "    \"NEGATIVE\": \"negative\",\n",
        "    \"POSITIVE\": \"positive\",\n",
        "    0: \"negative\",\n",
        "    1: \"positive\"\n",
        "}\n",
        "\n",
        "\n",
        "def compare_models(text):\n",
        "    # Fine-tuned SetFit model\n",
        "    ft_pred = fine_tuned_model.predict([text])[0]\n",
        "    ft_pred = int(ft_pred)  # Convert tensor to int\n",
        "\n",
        "    ft_label = label_map.get(ft_pred, str(ft_pred))\n",
        "\n",
        "    ft_proba = fine_tuned_model.predict_proba([text])[0]\n",
        "    ft_confidence = round(float(max(ft_proba)), 4)  # Convert tensor to float\n",
        "\n",
        "    ft_result = f\"Label: {ft_label}, Confidence: {ft_confidence}\"\n",
        "\n",
        "    # Pretrained pipeline model\n",
        "    pre_pred = pretrained_classifier(text)[0]\n",
        "    pre_label = label_map.get(pre_pred[\"label\"], pre_pred[\"label\"])\n",
        "    pre_score = round(pre_pred[\"score\"], 4)\n",
        "    pre_result = f\"Label: {pre_label}, Confidence: {pre_score}\"\n",
        "\n",
        "    return ft_result, pre_result\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## ðŸŽ¬ Compare Fine-Tuned(SetFit) vs Pretrained Models\")\n",
        "    gr.Markdown(\"Enter a movie review below to compare predictions.\")\n",
        "\n",
        "    input_text = gr.Textbox(label=\"Movie Review\", lines=4, placeholder=\"Type your review here...\")\n",
        "    classify_btn = gr.Button(\"Classify\")\n",
        "\n",
        "    with gr.Row():\n",
        "        ft_output = gr.Textbox(label=\"Fine-Tuned Model Prediction\", interactive=False)\n",
        "        pre_output = gr.Textbox(label=\"Pretrained Model Prediction\", interactive=False)\n",
        "\n",
        "    classify_btn.click(compare_models, inputs=input_text, outputs=[ft_output, pre_output])\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "id": "QpiPkqbVAjn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1fjRtyFoBSc8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}